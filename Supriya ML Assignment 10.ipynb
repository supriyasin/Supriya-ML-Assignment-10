{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0085af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Define the Bayesian interpretation of probability.\n",
    "\n",
    "\"\"\"The Bayesian interpretation of probability is a philosophical and mathematical framework for understanding \n",
    "   probability as a measure of uncertainty or belief in the context of incomplete information. It is named after\n",
    "   the Reverend Thomas Bayes, an 18th-century mathematician and theologian.\n",
    "\n",
    "   In the Bayesian view, probability is not seen as a fixed property of an event, but rather as a representation \n",
    "   of an individual's subjective degree of belief or confidence in the occurrence of that event. This interpretation\n",
    "   emphasizes the role of prior knowledge, evidence, and new information in updating one's beliefs.\n",
    "\n",
    "   The core principles of the Bayesian interpretation include:\n",
    "\n",
    "   1. Prior Probability (Prior Belief): Before observing any new evidence, an individual assigns an initial \n",
    "      probability (prior probability) to different possible outcomes based on their existing knowledge, \n",
    "      experiences, and beliefs.\n",
    "\n",
    "   2. Likelihood: As new evidence is obtained, the likelihood function quantifies the probability of observing\n",
    "      the evidence given each possible outcome. It essentially describes how well the data supports different\n",
    "      hypotheses.\n",
    "\n",
    "   3. Posterior Probability (Updated Belief): After incorporating the new evidence, the individual updates their\n",
    "      beliefs using Bayes' theorem. The posterior probability is the revised probability assigned to different \n",
    "      outcomes, taking into account both the prior probability and the likelihood of the evidence.\n",
    "\n",
    "   Mathematically, Bayes' theorem expresses this relationship:\n",
    "\n",
    "   \\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\( P(A|B) \\) is the posterior probability of event A given evidence B.\n",
    "   - \\( P(B|A) \\) is the likelihood of observing evidence B given event A.\n",
    "   - \\( P(A) \\) is the prior probability of event A.\n",
    "   - \\( P(B) \\) is the marginal likelihood of evidence B.\n",
    "\n",
    "   In summary, the Bayesian interpretation of probability offers a framework to update beliefs in a rational\n",
    "   and systematic manner as new evidence becomes available. It is widely used in fields such as statistics, \n",
    "   machine learning, and artificial intelligence, where uncertainty and the incorporation of new information \n",
    "   play crucial roles.\"\"\"\n",
    "\n",
    "#2. Define probability of a union of two events with equation.\n",
    "\n",
    "\"\"\"The probability of the union of two events, denoted as \\(P(A \\cup B)\\), is the probability that at least one\n",
    "   of the two events A or B occurs. Mathematically, it is expressed as:\n",
    "\n",
    "   \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\]\n",
    "\n",
    "   Where:\n",
    "   - \\( P(A) \\) is the probability of event A occurring.\n",
    "   - \\( P(B) \\) is the probability of event B occurring.\n",
    "   - \\( P(A \\cap B) \\) is the probability of the intersection of events A and B occurring (i.e., both events A \n",
    "     and B occurring simultaneously).\n",
    "\n",
    "   The equation for the probability of the union of two events reflects the fact that if we simply added \\(P(A)\\)\n",
    "   and \\(P(B)\\), we would be double-counting the probability of both events occurring together (i.e., \\(P(A \\cap B)\\)).\n",
    "   Therefore, we subtract \\(P(A \\cap B)\\) to correct for this overlap.\n",
    "\n",
    "   It's important to note that the equation assumes that events A and B are not mutually exclusive (i.e., they can\n",
    "   both occur together), as the subtraction of \\(P(A \\cap B)\\) accounts for the double counting. If events A and B\n",
    "   are mutually exclusive, meaning they cannot occur simultaneously, then the equation simplifies to \\(P(A \\cup B)\n",
    "   = P(A) + P(B)\\).\"\"\"\n",
    "\n",
    "#3. What is joint probability? What is its formula?\n",
    "\n",
    "\"\"\"Joint probability refers to the probability of two or more events occurring simultaneously. It provides a way\n",
    "   to quantify the likelihood of the co-occurrence of multiple events. The joint probability of events A and B \n",
    "   is denoted as \\(P(A \\cap B)\\).\n",
    "\n",
    "   The formula for calculating the joint probability of two events A and B is straightforward:\n",
    "\n",
    "   \\[ P(A \\cap B) = P(A) \\times P(B|A) \\]\n",
    "\n",
    "   Where:\n",
    "   - \\( P(A) \\) is the probability of event A occurring.\n",
    "   - \\( P(B|A) \\) is the conditional probability of event B occurring given that event A has occurred.\n",
    "\n",
    "   In words, the joint probability of events A and B is the product of the probability of event A and the\n",
    "   conditional probability of event B given that event A has occurred. This formula reflects the idea that\n",
    "   the joint probability captures the likelihood of both events A and B happening together.\n",
    "\n",
    "   If events A and B are independent, meaning that the occurrence of one event does not affect the occurrence \n",
    "   of the other, then the joint probability simplifies to the product of the individual probabilities: \\(P(A \n",
    "   \\cap B) = P(A) \\times P(B)\\). In this case, the occurrence of event A does not influence the probability of\n",
    "   event B, and vice versa.\n",
    "\n",
    "   Joint probabilities are fundamental in various areas of probability theory, statistics, and data analysis, \n",
    "   such as in calculating the probability of complex events or in building probabilistic models.\"\"\"\n",
    "\n",
    "#4. What is chain rule of probability?\n",
    "\n",
    "\"\"\"The chain rule of probability, also known as the multiplication rule, is a fundamental principle in probability\n",
    "   theory that allows us to calculate the probability of the joint occurrence of multiple events by breaking it\n",
    "   down into a sequence of conditional probabilities. This rule is particularly useful when dealing with complex\n",
    "   events that can be decomposed into simpler, sequential events.\n",
    "\n",
    "    Mathematically, the chain rule states that for a sequence of events \\(A_1, A_2, \\ldots, A_n\\), the joint \n",
    "    probability of all these events occurring can be calculated as the product of conditional probabilities:\n",
    "\n",
    "   \\[ P(A_1 \\cap A_2 \\cap \\ldots \\cap A_n) = P(A_1) \\times P(A_2 | A_1) \\times P(A_3 | A_1 \\cap A_2) \\times \n",
    "   \\ldots \\times P(A_n | A_1 \\cap A_2 \\cap \\ldots \\cap A_{n-1}) \\]\n",
    "\n",
    "   In other words, the joint probability of all the events is the product of the probabilities of each event\n",
    "   occurring given that all the previous events have occurred.\n",
    "\n",
    "   The chain rule becomes especially valuable when dealing with events that can be causally related or that follow\n",
    "   a specific order of occurrence. By breaking down the joint probability into a sequence of conditional probabilities,\n",
    "   the rule allows us to analyze complex scenarios in a more manageable and systematic way.\n",
    "\n",
    "   The chain rule is a fundamental concept in probability theory and is essential in building probabilistic models, \n",
    "   such as Bayesian networks, where events are connected in a directed acyclic graph representing causal relationships\n",
    "   or dependencies.\"\"\"\n",
    "\n",
    "#5. What is conditional probability means? What is the formula of it?\n",
    "\n",
    "\"\"\"Conditional probability is a measure of the likelihood that an event will occur given that another event has \n",
    "   already occurred. In other words, it quantifies how the probability of one event is affected by the occurrence \n",
    "   of another event. Conditional probability is denoted as \\(P(A|B)\\), which reads as \"the probability of event \n",
    "   A given event B.\"\n",
    "\n",
    "   The formula for calculating conditional probability is:\n",
    "\n",
    "   \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\( P(A|B) \\) is the conditional probability of event A occurring given event B has occurred.\n",
    "   - \\( P(A \\cap B) \\) is the joint probability of events A and B occurring simultaneously.\n",
    "   - \\( P(B) \\) is the probability of event B occurring.\n",
    "\n",
    "   In words, the formula states that the conditional probability of event A given event B is the ratio of the \n",
    "   joint probability of both events A and B occurring to the probability of event B occurring.\n",
    "\n",
    "   Conditional probability allows us to update our beliefs about the likelihood of an event based on new information \n",
    "   or evidence provided by the occurrence of another event. It is a key concept in various fields such as statistics,\n",
    "   probability theory, and machine learning, and it plays a crucial role in understanding dependencies and \n",
    "   relationships between events.\"\"\"\n",
    "\n",
    "#6. What are continuous random variables?\n",
    "\n",
    "\"\"\"Continuous random variables are a type of random variable in probability theory that can take on any valu\n",
    "   within a specified range, often an interval on the real number line. Unlike discrete random variables, \n",
    "   which can only take distinct, separate values, continuous random variables have an infinite number of\n",
    "   possible values within their range. These values are typically represented by points on a continuous spectrum.\n",
    "\n",
    "   Examples of continuous random variables include measurements such as height, weight, time, temperature, and \n",
    "   distance. These variables can take on any value within a certain range, and there can be an infinite number \n",
    "   of potential values between any two points.\n",
    "\n",
    "   Mathematically, continuous random variables are described by probability density functions (PDFs) rather than\n",
    "   probability mass functions (PMFs), which are used for discrete random variables. The PDF describes the likelihood\n",
    "   of the variable falling within a specific range of values. The area under the PDF curve over a certain interval\n",
    "   corresponds to the probability of the random variable falling within that interval.\n",
    "\n",
    "   The concept of continuous random variables is essential in various fields such as statistics, physics, \n",
    "   engineering, and economics, where measurements and observations often lead to outcomes that lie on a \n",
    "   continuous scale.\"\"\"\n",
    "\n",
    "#7. What are Bernoulli distributions? What is the formula of it?\n",
    "\n",
    "\"\"\"The Bernoulli distribution is a discrete probability distribution that describes a random experiment with\n",
    "   two possible outcomes: success (usually denoted as 1) or failure (usually denoted as 0). It models situations\n",
    "   where there is a binary or dichotomous outcome. The distribution is named after Jacob Bernoulli, a Swiss \n",
    "   mathematician, and is often used as a building block for more complex distributions.\n",
    "\n",
    "   The Bernoulli distribution has a single parameter, \\(p\\), which represents the probability of success. \n",
    "   The probability mass function (PMF) of the Bernoulli distribution is given by:\n",
    "\n",
    "   \\[ P(X = x) = \n",
    "   \\begin{cases} \n",
    "    p & \\text{if } x = 1 \\\\\n",
    "    1 - p & \\text{if } x = 0 \n",
    "   \\end{cases} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\(X\\) is the random variable that follows the Bernoulli distribution.\n",
    "   - \\(x\\) can take values 0 (failure) or 1 (success).\n",
    "   - \\(p\\) is the probability of success (i.e., the probability that \\(X = 1\\)).\n",
    "\n",
    "   In this distribution, the mean (expected value) is \\(E(X) = p\\) and the variance is \\(Var(X) = p(1 - p)\\).\n",
    "\n",
    "   The Bernoulli distribution is a simple but important concept in probability theory, serving as the basis for\n",
    "   understanding and modeling various binary events or scenarios, such as coin flips, success/failure outcomes,\n",
    "   and yes/no questions. It forms the foundation for more complex distributions, like the binomial distribution \n",
    "   and the geometric distribution.\"\"\"\n",
    "\n",
    "#8. What is binomial distribution? What is the formula?\n",
    "\n",
    "\"\"\"The binomial distribution is a discrete probability distribution that describes the number of successes in a\n",
    "   fixed number of independent Bernoulli trials. A Bernoulli trial is an experiment with two possible outcomes: \n",
    "   success (usually denoted as 1) or failure (usually denoted as 0). The binomial distribution models situations\n",
    "   where you repeat the same experiment multiple times and count the number of successes.\n",
    "\n",
    "   The binomial distribution has two parameters: \\(n\\) and \\(p\\):\n",
    "   - \\(n\\) represents the number of trials or experiments.\n",
    "   - \\(p\\) represents the probability of success in each individual trial.\n",
    "\n",
    "   The probability mass function (PMF) of the binomial distribution is given by:\n",
    "\n",
    "   \\[ P(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\(X\\) is the random variable that follows the binomial distribution.\n",
    "   - \\(k\\) is the number of successes you're interested in (can range from 0 to \\(n\\)).\n",
    "   - \\(\\binom{n}{k}\\) represents the number of combinations of \\(n\\) trials taken \\(k\\) at a time, often \n",
    "     denoted as \"n choose k\" or the binomial coefficient.\n",
    "   - \\(p\\) is the probability of success in each individual trial.\n",
    "   - \\(1 - p\\) is the probability of failure in each individual trial.\n",
    "\n",
    "   The mean (expected value) of a binomial distribution is \\(E(X) = np\\), and the variance is \\(Var(X) = np(1 - p)\\).\n",
    "\n",
    "   The binomial distribution is used to model scenarios such as coin flips, where you want to know the probability\n",
    "   of getting a certain number of heads in a fixed number of tosses, or in situations involving success/failure\n",
    "   outcomes that are repeated independently multiple times.\"\"\"\n",
    "\n",
    "#9. What is Poisson distribution? What is the formula?\n",
    "\n",
    "\"\"\"The Poisson distribution is a discrete probability distribution that models the number of events occurring in\n",
    "   a fixed interval of time or space when the events are rare and randomly distributed. It's often used to \n",
    "   describe rare events or occurrences that happen with a low probability but over a large number of trials. \n",
    "   The distribution is named after the French mathematician Sim√©on Denis Poisson.\n",
    "\n",
    "   The Poisson distribution has one parameter, \\(\\lambda\\), which represents the average rate of occurrence of \n",
    "   the events within the given interval.\n",
    "\n",
    "   The probability mass function (PMF) of the Poisson distribution is given by:\n",
    "\n",
    "   \\[ P(X = k) = \\frac{e^{-\\lambda} \\cdot \\lambda^k}{k!} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\(X\\) is the random variable that follows the Poisson distribution.\n",
    "   - \\(k\\) is the number of events you're interested in.\n",
    "   - \\(e\\) is the base of the natural logarithm.\n",
    "   - \\(\\lambda\\) is the average rate of events occurring in the interval.\n",
    "   - \\(k!\\) is the factorial of \\(k\\).\n",
    "\n",
    "   The mean (expected value) and variance of a Poisson distribution are both equal to \\(\\lambda\\), i.e., \\(E(X)\n",
    "   = \\lambda\\) and \\(Var(X) = \\lambda\\).\n",
    "\n",
    "   The Poisson distribution is used in various fields to model a wide range of phenomena, such as the number \n",
    "   of phone calls received by a call center in a given time period, the number of accidents occurring at an\n",
    "   intersection in a day, or the number of emails received in an hour. It's particularly suitable for cases\n",
    "   where rare events are being counted in a fixed interval, and the events are independent of each other.\"\"\"\n",
    "\n",
    "#10. Define covariance.\n",
    "\n",
    "\"\"\"Covariance is a statistical measure that quantifies the degree to which two random variables change together.\n",
    "   It indicates the extent to which changes in one variable are associated with changes in another variable. \n",
    "   In other words, covariance measures the linear relationship between two variables. \n",
    "\n",
    "   Mathematically, the covariance between two random variables \\(X\\) and \\(Y\\) is calculated using the following formula:\n",
    "\n",
    "   \\[ \\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n} \\]\n",
    "\n",
    "   Where:\n",
    "   - \\(x_i\\) and \\(y_i\\) are individual observations of variables \\(X\\) and \\(Y\\) respectively.\n",
    "   - \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means (averages) of variables \\(X\\) and \\(Y\\) respectively.\n",
    "   - \\(n\\) is the number of observations.\n",
    "\n",
    "  The covariance can take various values: \n",
    "  - If the covariance is positive, it indicates that the variables tend to increase together (when one is above \n",
    "    its mean, the other is also above its mean).\n",
    "  - If the covariance is negative, it indicates that the variables tend to move in opposite directions (when \n",
    "    one is above its mean, the other is below its mean).\n",
    "  - If the covariance is close to zero, it suggests that there is little to no linear relationship between\n",
    "    the variables.\n",
    "\n",
    "  However, interpreting the covariance value alone can be challenging, as it does not provide a standardized \n",
    "  measure of association that is easily interpretable or comparable across different datasets. For this reason,\n",
    "  the concept of correlation is often used, which is derived from covariance and is scaled to a range between -1\n",
    "  and 1 to provide a more interpretable and standardized measure of linear relationship.\"\"\"\n",
    "\n",
    "#11. Define correlation\n",
    "\n",
    "\"\"\"Correlation refers to a statistical measure that describes the extent to which two variables change together. \n",
    "   In other words, it quantifies the strength and direction of the relationship between two sets of data points.\n",
    "   Correlation does not imply causation; it only indicates whether changes in one variable are associated with\n",
    "   changes in another.\n",
    "\n",
    "   There are two main types of correlation:\n",
    "\n",
    "   1. Positive Correlation: In this type of correlation, as one variable increases, the other variable also \n",
    "      tends to increase. Similarly, as one variable decreases, the other variable tends to decrease. A positive \n",
    "      correlation is represented by a correlation coefficient that ranges from 0 to +1, where a value closer\n",
    "      to +1 indicates a stronger positive relationship.\n",
    "\n",
    "   2. Negative Correlation: In this type of correlation, as one variable increases, the other variable tends to\n",
    "      decrease, and vice versa. A negative correlation is represented by a correlation coefficient that ranges \n",
    "      from 0 to -1, where a value closer to -1 indicates a stronger negative relationship.\n",
    "\n",
    "   The correlation coefficient is a numerical value that quantifies the strength and direction of the correlation\n",
    "   between two variables. It is typically calculated using mathematical formulas such as Pearson's correlation \n",
    "   coefficient (for linear relationships) or Spearman's rank correlation coefficient (for monotonic relationships).\n",
    "\n",
    "   It's important to note that correlation does not necessarily imply a causal relationship between the variables. \n",
    "   Even if two variables are highly correlated, it doesn't mean that changes in one variable cause changes in the \n",
    "   other. Correlation can be affected by various factors, including chance, confounding variables, and the presence\n",
    "   of other underlying relationships.\"\"\"\n",
    "\n",
    "#12. Define sampling with replacement. Give example.\n",
    "\n",
    "\"\"\"Sampling with replacement is a sampling technique used in statistics and probability, where a member of a\n",
    "   population or a data point is selected from a dataset, and after it's selected, it's put back into the \n",
    "   dataset before the next selection. This means that the same data point can be selected more than once\n",
    "   during the sampling process. Each selection is independent of the previous selections.\n",
    "\n",
    "   Example of sampling with replacement:\n",
    "\n",
    "   Imagine you have a bag containing colored balls: 5 red balls, 3 blue balls, and 2 green balls. If you were\n",
    "   to perform sampling with replacement, you would randomly select a ball from the bag, record its color, and\n",
    "   then put the ball back in the bag before making the next selection.\n",
    "\n",
    "   Let's simulate this process:\n",
    " \n",
    "   1. We reach into the bag and randomly select a ball. Let's say you pick a red ball. We note down \"red\" \n",
    "      and put the red ball back in the bag.\n",
    "\n",
    "   2. We reach into the bag again and randomly select a ball. This time, we might pick a blue ball. We note\n",
    "      down \"blue\" and put the blue ball back in the bag.\n",
    "\n",
    "   3. On the next draw, you might pick a red ball again, even though you already picked one earlier. We put \n",
    "      the red ball back in the bag after noting its color.\n",
    "\n",
    "   4. This process continues for a certain number of draws.\n",
    "\n",
    "   With sampling with replacement, the probabilities of selecting each type of ball remain the same for every \n",
    "   draw, regardless of the previous selections. In the example above, the probabilities of picking a red ball, \n",
    "   a blue ball, or a green ball remain constant throughout the process. This is in contrast to sampling without  \n",
    "   replacement, where the probabilities change with each draw because the available pool of items is getting \n",
    "   smaller after each selection.\"\"\"\n",
    "\n",
    "#13. What is sampling without replacement? Give example.\n",
    "\n",
    "\"\"\"Sampling without replacement is a sampling technique used in statistics and probability, where a member of a \n",
    "   population or a data point is selected from a dataset, and once it's selected, it's not put back into the\n",
    "   dataset before the next selection. This means that each data point can only be selected once during the \n",
    "   sampling process, and the available pool of items decreases with each selection.\n",
    "\n",
    "   Example of sampling without replacement:\n",
    "\n",
    "   Imagine you have a deck of 52 playing cards, and you want to select 3 cards without replacement.\n",
    "\n",
    "   1. We draw the first card randomly from the deck. Let's say you draw a \"7 of Hearts.\" We note down the card\n",
    "      we drew.\n",
    "\n",
    "   2. For the second draw, we have 51 cards left in the deck since you didn't replace the first card. we draw \n",
    "      another card. Let's say you draw the \"King of Spades.\" We note down the second card.\n",
    "\n",
    "   3. For the third and final draw, you have 50 cards left in the deck. We draw the last card, which happens to \n",
    "      be the \"3 of Diamonds.\"\n",
    " \n",
    "   In this example, each card drawn is unique, and the available pool of cards gets smaller with each draw. \n",
    "   The probabilities of drawing specific cards change with each selection because there are fewer cards\n",
    "   remaining in the deck.\n",
    "\n",
    "   Sampling without replacement is often used when you want to maintain the representativeness of the sample \n",
    "   and avoid double-counting or introducing bias due to repeated selections. It's commonly employed in situations \n",
    "   where the population size is relatively small compared to the sample size or when you're dealing with \n",
    "   finite resources.\"\"\"\n",
    "\n",
    "#14. What is hypothesis? Give example.\n",
    "\n",
    "\"\"\"A hypothesis is a testable and specific statement or proposition that suggests a possible explanation for\n",
    "   a phenomenon or a relationship between variables. It serves as a basis for scientific research and \n",
    "   experimentation, as it can be empirically tested and either supported or refuted through observations\n",
    "   and data analysis.\n",
    "\n",
    "   A hypothesis typically has two main components:\n",
    "\n",
    "   1. Null Hypothesis (H0): This is the default hypothesis that there is no significant effect or relationship.\n",
    "      It often represents the idea that any observed differences or relationships in the data are due to chance.\n",
    "\n",
    "   2. Alternative Hypothesis (H1 or Ha): Also known as the research hypothesis, this is the statement that \n",
    "      contradicts the null hypothesis. It suggests a specific effect, relationship, or difference that is being \n",
    "      investigated.\n",
    "\n",
    "   Example of a hypothesis:\n",
    "\n",
    "   Let's say a researcher is interested in studying the effect of a new fertilizer on the growth of tomato plants. \n",
    "   They might formulate the following hypotheses:\n",
    "\n",
    "   - Null Hypothesis (H0): The new fertilizer has no significant effect on the growth of tomato plants.\n",
    "   - Alternative Hypothesis (H1): The new fertilizer leads to a significant increase in the growth of tomato plants.\n",
    "\n",
    "   In this example, the null hypothesis states that there is no effect, while the alternative hypothesis proposes \n",
    "   a specific effect (increase in growth). The researcher would then conduct an experiment, collect data on tomato\n",
    "   plant growth with and without the new fertilizer, and analyze the results to determine whether the data supports \n",
    "   or refutes the hypotheses.\n",
    "\n",
    "   Hypotheses are essential in the scientific method because they guide research efforts, help researchers focus\n",
    "   their investigations, and provide a framework for drawing conclusions based on empirical evidence. The process\n",
    "   of testing hypotheses and drawing conclusions based on evidence is fundamental to advancing scientific knowledge.\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
